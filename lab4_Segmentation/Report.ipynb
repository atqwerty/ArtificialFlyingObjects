{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "essential-gregory",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-adaptation",
   "metadata": {},
   "source": [
    "**Name:** Markitanov Denis \\\n",
    "**Date:** 19.12.21\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This lab is an introduction to image segmentation. In this lab I've observed how different parameters and layers influence the result of image segmentation model\n",
    "\n",
    "\n",
    "## Result\n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Task 1:\n",
    "\n",
    "As in the last lab - high accuracy does not imply good performance.\n",
    "\n",
    "#### Task 2:\n",
    "\n",
    "I've used Intersection Over Union as it is built in torchmetrics.\n",
    "\n",
    "For the Loss I've used the Dice coefficient. However I was not able to find an implementation in the pytorch library. Thus, I found a solution from here (https://kornia.readthedocs.io/en/v0.1.2/_modules/torchgeometry/losses/dice.html)_. I changed the code by removing unecessary parts (error checks)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Task 1:\n",
    "\n",
    "I was able to overfit model by increasing the number of layers and kernel size. I have two architectures with different channels. The model performed with 18% accuracy on the test data\n",
    "\n",
    "#### Task 2:\n",
    "\n",
    "I used 3x3 kernel, 1 padding, and different sizes of channels. Model was able to achieve around 40% of accuracy on the test data. Which is not a good accuracy, but a significant difference between this and overfit model\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "For this experiment I decreased learning rate and increased the amount of epochs. I explicitly decided to not to change the architecture as I was satisfied with (poor) performance of the last architecture (its better than 1st architecture)\n",
    "\n",
    "The results showed that the model achieved 45% in accuracy, which is not a good result, but it is an improvement.\n",
    "\n",
    "### Augmentation\n",
    "\n",
    "In this section things get more interesting. I've implemented additional method for the transforms - RandomVertFlip. Which is a copy of RandomHorizontalFlip, but vertical (changed 2 lines of code). Applying both horizontal and vertical flips incresed the accuracy by almost 30%. The accuracy is 72%. Which is a collosal improvement, while not the best possible performance.\n",
    "\n",
    "#### Q1 and Q2 answer:\n",
    "\n",
    "Data augmentation did improve the model. I used 2nd model architecture (with 40%) performance to see if model is sensetive to data augmentation or tuning. Turns out - data augmentation (as well as kernel size, padding, and channel sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fce83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
