{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Exercise II: Regression</h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to look at a regression problem. The data as described above (regr1) consists of 6 inputs (features) and one output (target) value. As for previous examples a new data set is generated each time you call the *regr1* function. To get exactly the same data set between different calls, use a fixed seed. New for this problem is that one can also control the amount of noise added to the target value. We are going to use a relatively small training dataset (\\~250) and a larger validation dataset (\\~1000) to get a more robust estimation of the generalization performance. For regression problems we also need new performance measures. The *stats_reg* function will give you two such measures:\n",
    "* MSE = mean squared error (low error mean good performance)\n",
    "* CorrCoeff = Pearson correlation coefficient for the scatter plot between predicted and true values.\n",
    "\n",
    "# Data \n",
    "## regr1\n",
    "There is also a synthetic regression problem, called *regr1*. It has 6 inputs (independent variables) and one output variable (dependent variable). It is generated according to the following formula:  \n",
    "\n",
    "$\\qquad d = 2x_1 + x_2x_3^2 + e^{x_4} + 5x_5x_6 + 3\\sin(2\\pi x_6) + \\alpha\\epsilon$  \n",
    "    \n",
    "where $\\epsilon$ is added normally distributed noise and $\\alpha$ is a parameter controlling the size of the added noise. Variables $x_1,...,x_4$ are normally distrubuted with zero mean and unit variance, whereas $x_5, x_6$ are uniformly distributed ($[0,1]$). The target value $d$ has a non-linear dependence on ***x***.\n",
    "\n",
    "\n",
    "# Tasks\n",
    "\n",
    "## Task 1\n",
    "Use 250 data points for training and about 1000 for validation and **no** added noise. Train an MLP to predict the target output. If you increase the complexity of the model (e.g. number of hidden nodes) you should be able to reach a very small training error. You will also most likely see that the validation error decreases as you increase the complexity or at least no clear sign of overtraining. \n",
    "\n",
    "**Note:** As with previous examples you may need to tune the optimization parameters to make sure that you have \"optimal\" training. That is, increase or decrease the learningrate, possibly train longer times (increase *epochs*) and change the *batch_size* parameter.\n",
    "\n",
    "**TODO:** Even though the validation error is most likely still larger than the training error why do we not see any overtraining of the model? (Hint: What is it that typically causes overfitting?)\n",
    "\n",
    "## Task 2\n",
    "Use the same training and validation data sets as above, but add 0.4 units of noise (set the second parameter when calling the *regr1* function to 0.4 for both training and validation). Now train again, starting with a \"small\" model and increase the number of hidden nodes as you monitor the validation result for each model. Make a note of the validation error you obtained a this point!\n",
    "\n",
    "**TODO:** How many nodes do you have for opitimal validation performance, i.e. more hidden nodes results in overtraining?\n",
    "\n",
    "## Task 3\n",
    "Instead of using the number of hidden nodes to control the complexity it is often better to use a regularization term added to the error function. You are now going to control the complexity by adding a *L2* regularizer (see the \"INPUT\" dictionary in the cell). You should modify this value until you find the \"near optimal\" validation performance. Use 15 hidden nodes. \n",
    "\n",
    "**TODO:** Give the L2 value that you found to give \"optimal\" validation performance and compare with the result from  question 7 (optimal performance).\n",
    "\n",
    "## Task 4\n",
    "**TODO:** Summarize your findings in a few sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows us to edit imported files without restarting the kernel for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Hacky solution to access the global utils package\n",
    "import sys,os\n",
    "sys.path.append(os.path.dirname(os.path.realpath('')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from config import LabConfig\n",
    "from dataset import MLPData\n",
    "from utils.model import Model\n",
    "from utils.progressbar import LitProgressBar\n",
    "from utils.model import Model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils import (\n",
    "    plot,\n",
    "    progressbar\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LabConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the MLP model\n",
    "\n",
    "This cell defines the MLP model. There are a number of parameters that is needed to \n",
    "define a model. Here is a list of them: **Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                inp_dim=None,         \n",
    "                hidden_nodes=1, # number of nodes in hidden layer\n",
    "                num_out=None,\n",
    "                **kwargs\n",
    "            ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(inp_dim, hidden_nodes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_nodes, num_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        return torch.tanh(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that allow us to convert numpy to pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy2Dataloader(x,y, batch_size=25, num_workers=10,**kwargs):\n",
    "    return DataLoader(\n",
    "        TensorDataset(\n",
    "            torch.from_numpy(x).float(), \n",
    "            torch.from_numpy(y).unsqueeze(1).float()\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation data\n",
    "x_train, d_train = MLPData.regr1(250, 0.0) # 250 data points with no noise\n",
    "x_val, d_val = MLPData.regr1(1000, 0.0)\n",
    "\n",
    "# Here we need to normalize the target values\n",
    "norm_m = d_train.mean(axis=0)\n",
    "norm_s = d_train.std(axis=0)\n",
    "d_train = (d_train - norm_m) / norm_s\n",
    "\n",
    "# We use the same normalization for the validation data.\n",
    "d_val = (d_val - norm_m) / norm_s\n",
    "\n",
    "train_loader = numpy2Dataloader(x_train,d_train)\n",
    "val_loader =  numpy2Dataloader(x_val,d_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Setup our local config that should be used for the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'max_epochs':20,\n",
    "    'model_params':{\n",
    "        'inp_dim':x_train.shape[1],         \n",
    "        'hidden_nodes':1,   # activation functions for the hidden layer\n",
    "        'num_out':1 # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "    },\n",
    "    'criterion':torch.nn.MSELoss(), # error function\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, put everything together and call on the trainers fit method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:730: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:394: UserWarning: The number of training samples (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [9/10] {'loss': '1.17'}}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1805: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 [10/10] {'loss': '0.607'}\r"
     ]
    }
   ],
   "source": [
    "model = Model(MLP(**config[\"model_params\"]),**config)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=cfg.GPU,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[LitProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloaders=val_loader\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No False Positive cant be zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2234093/2122589188.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stud1/d/denmar20/ArtificialFlyingObjects/utils/plot.py\u001b[0m in \u001b[0;36mstats_class\u001b[0;34m(x, y, label, model)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnof_p\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No False Positive cant be zero\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnof_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No False Positive cant be zero"
     ]
    }
   ],
   "source": [
    "plot.stats_class(x_train, d_train, 'Training', model)\n",
    "plot.stats_class(x_val, d_val, 'Validation', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58561936,  0.98602412,  0.36361025,  2.06383404,  0.06505802,\n",
       "       -0.82929667,  0.08070941,  0.19945641,  0.53680017,  0.57401835,\n",
       "        0.15227812,  0.69215745,  0.23303388, -1.06518951, -0.64961266,\n",
       "       -1.48501043, -1.15671963,  0.82317982, -0.32526958,  0.61388274,\n",
       "        1.57067767,  0.48804659, -1.08018679,  0.92410638, -0.3922601 ,\n",
       "       -1.11296605,  2.15298778,  2.49156797,  1.68488207,  0.35771921,\n",
       "        0.28726033,  0.7799129 ,  0.67479483, -0.76973385,  0.31020062,\n",
       "       -0.98434003,  2.34804886,  0.08251796,  0.93566323, -0.38817927,\n",
       "        0.75242135,  0.23810264, -0.14020385,  0.27518528, -1.40467257,\n",
       "        1.75371091,  1.98188934,  0.55465896,  0.36868815,  0.23555466,\n",
       "       -0.61289734, -1.38042114,  0.01740044, -0.35260033, -1.52687007,\n",
       "       -1.35077443,  0.06172444,  0.60179745, -1.69715578, -0.05016608,\n",
       "        0.92625793, -1.2223584 ,  0.0518899 ,  0.42471049, -1.179242  ,\n",
       "       -0.5978531 , -1.15384908, -1.49230972,  0.38789304, -0.01485959,\n",
       "       -1.37817081, -0.84915885, -1.92364097,  0.32105148, -0.03572898,\n",
       "        0.86944168,  1.56490707,  0.09247122,  0.52147613,  2.2632428 ,\n",
       "        0.2322444 , -1.43853432, -0.74739585,  1.18686979, -1.34265105,\n",
       "        0.50161766, -0.10562073,  0.19632974, -1.05420921, -0.55866684,\n",
       "       -2.77928756, -0.06194623, -0.38717653,  0.59880596, -0.17404356,\n",
       "        0.1152892 ,  0.34243158,  1.00020503,  1.04850561, -2.57972513,\n",
       "       -1.00141323, -1.04166973, -0.99904703,  0.30041931,  0.24805943,\n",
       "       -1.50895091,  0.45835199,  0.41455276,  0.04493021, -0.45169295,\n",
       "       -0.10246228, -0.23180139, -0.17578832, -2.01552495, -0.60740064,\n",
       "       -0.70620971, -0.00533946, -0.61154505, -1.69358927, -1.61743025,\n",
       "        0.71308636,  0.055931  , -1.93522329,  0.06852742, -0.14992057,\n",
       "       -1.5199562 , -1.34099029, -0.17646476,  0.53484714, -1.57064812,\n",
       "        0.26919719,  0.46840811,  0.06723865,  0.94398894,  1.40297815,\n",
       "        1.15922843,  0.41290362,  0.22539283, -0.22787506,  0.41862057,\n",
       "       -0.55227649,  2.14300407, -0.15262645,  1.6768504 ,  0.41926994,\n",
       "       -3.38133597, -0.91598847,  0.17026629, -0.7374182 ,  1.15451983,\n",
       "       -0.20028064, -1.01703509, -0.51261087,  1.60313949,  0.41054689,\n",
       "        0.91243241, -1.32470982, -0.31118371,  0.42546882,  0.17133369,\n",
       "        0.73787481, -0.16832179, -0.56111614, -0.57934773,  1.41773263,\n",
       "        2.76274168, -1.28169071, -0.96522888, -0.14644827, -0.5090713 ,\n",
       "       -0.37106593,  0.11445757, -0.51082277,  0.03948509,  0.8555133 ,\n",
       "       -0.66170127,  0.77518561,  0.67479386, -0.10250983, -0.17235133,\n",
       "        0.59065801,  2.29069836, -0.47744162,  1.77992657,  0.40137023,\n",
       "        0.1727202 ,  0.66321306, -0.79830859,  0.39091754, -0.42201161,\n",
       "        0.32688611,  1.2157311 ,  0.63240814,  0.42936358,  1.00390784,\n",
       "        0.08623483,  0.00682121, -0.53463838,  0.11569346, -0.42041603,\n",
       "        1.85960718, -0.59625705, -0.83268203,  0.53146968, -0.24079385,\n",
       "        0.90311682,  1.25272791, -0.49165589,  0.61702859, -0.54179376,\n",
       "       -0.50093472, -0.61210826, -1.11288929, -1.60727203, -1.59137595,\n",
       "       -1.60420814,  0.79560312, -0.22130723, -0.21200148, -0.2165816 ,\n",
       "        1.35603487,  0.55664106,  0.82743654,  0.84213523, -1.58516738,\n",
       "        0.0909626 ,  0.36005987,  0.20892899,  0.80582787, -0.29713724,\n",
       "       -0.5838416 , -0.42255569,  2.48998086, -0.18980227, -0.94037252,\n",
       "       -0.75703844,  0.40676455, -1.08015045,  0.94147674,  0.02530321,\n",
       "        0.7759022 , -0.55728469,  1.02607472,  0.73892449,  1.57725526,\n",
       "        0.62176086, -0.57522464,  0.40613421, -0.65090129, -2.00861551])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(config_, train_loader_, val_loader_):\n",
    "    model = Model(MLP(**config_[\"model_params\"]),**config_)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "                max_epochs=config['max_epochs'], \n",
    "                gpus=cfg.GPU,\n",
    "                logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "                callbacks=[LitProgressBar()],\n",
    "                progress_bar_refresh_rate=1,\n",
    "                weights_summary=None, # Can be None, top or full\n",
    "                num_sanity_val_steps=10,   \n",
    "            )\n",
    "    trainer.fit(\n",
    "        model, \n",
    "        train_dataloader=train_loader_,\n",
    "        val_dataloaders=val_loader_\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:730: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 [10/10] {'loss': '0.244'}\r"
     ]
    }
   ],
   "source": [
    "# Task 1 copy of code with changes\n",
    "\n",
    "config = {\n",
    "    'max_epochs':150,\n",
    "    'model_params':{\n",
    "        'inp_dim':x_train.shape[1],         \n",
    "        'hidden_nodes':400,   # activation functions for the hidden layer\n",
    "        'num_out':1 # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "    },\n",
    "    'criterion':torch.nn.MSELoss(), # error function\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.01,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "model_eval(config, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 1\n",
      "Epoch 150 [10/10] {'loss': '1'}}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 10\n",
      "Epoch 150 [10/10] {'loss': '0.35'}}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 20\n",
      "Epoch 150 [10/10] {'loss': '0.323'}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 50\n",
      "Epoch 150 [10/10] {'loss': '0.242'}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 100\n",
      "Epoch 150 [10/10] {'loss': '0.245'}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 400\n",
      "Epoch 150 [10/10] {'loss': '0.239'}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 1000\n",
      "Epoch 150 [10/10] {'loss': '0.217'}\r"
     ]
    }
   ],
   "source": [
    "x_train, d_train = MLPData.regr1(250, 0.4) # 250 data points with no noise\n",
    "x_val, d_val = MLPData.regr1(1000, 0.4)\n",
    "\n",
    "# Here we need to normalize the target values\n",
    "norm_m = d_train.mean(axis=0)\n",
    "norm_s = d_train.std(axis=0)\n",
    "d_train = (d_train - norm_m) / norm_s\n",
    "\n",
    "# We use the same normalization for the validation data.\n",
    "d_val = (d_val - norm_m) / norm_s\n",
    "\n",
    "train_loader = numpy2Dataloader(x_train,d_train)\n",
    "val_loader =  numpy2Dataloader(x_val,d_val)\n",
    "\n",
    "for hidden_nodes in [1, 10, 20, 50, 100, 400, 1000]:\n",
    "    print('hidden_nodes: ' + str(hidden_nodes))\n",
    "    config = {\n",
    "        'max_epochs':150,\n",
    "        'model_params':{\n",
    "            'inp_dim':x_train.shape[1],         \n",
    "            'hidden_nodes':hidden_nodes,   # activation functions for the hidden layer\n",
    "            'num_out':1 # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "        },\n",
    "        'criterion':torch.nn.MSELoss(), # error function\n",
    "        'optimizer':{\n",
    "            \"type\":torch.optim.Adam,\n",
    "            \"args\":{\n",
    "                \"lr\":0.01,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    model_eval(config, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([25, 1])) that is different to the input size (torch.Size([25, 250])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 1000{'loss': '0.444'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 [10/10] {'loss': '0.519'}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 1000\n",
      "hidden_nodes: 1000{'loss': '0.57'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 [10/10] {'loss': '0.928'}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 [10/10] {'loss': '1'}}\r"
     ]
    }
   ],
   "source": [
    "# Task 3\n",
    "\n",
    "for l2 in [0.001, 0.005, 0.01, 0.05, 0.09]:\n",
    "    print('hidden_nodes: ' + str(hidden_nodes))\n",
    "    config = {\n",
    "    'max_epochs':100,\n",
    "    'model_params':{\n",
    "        'inp_dim':x_train.shape[1],         \n",
    "        'hidden_nodes':15,   # activation functions for the hidden layer\n",
    "        'num_out':x_train.shape[0] # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "    },\n",
    "    'criterion':torch.nn.MSELoss(), # error function\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.015,\n",
    "            'weight_decay': l2,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    model_eval(config, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ccf88e37874d44b4dfe33c31e1bb4a10ca4e414e0a68744582aebd290f71bcd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
