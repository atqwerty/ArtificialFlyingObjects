{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Exercise III:<br> Image Classification using CNNs</h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the *fourth* lab for Deep Learning!\n",
    "\n",
    "In this lab an CNN network to classify RGB images. Image classification refers to classify classes from images. This labs the *dataset* consist of multiple images where each image have a target label for classification.\n",
    "\n",
    "All **tasks** include **TODO's** thare are expected to be done before the deadline. The highlighted **Question's** should be answered in the report. Keep the answers separated so it is easy to read for the grading. Some sections include asserts or an expected result to give a and expected results are given. Some sections does not contain any **TODO's** but is good to understand them. \n",
    "\n",
    "For the **report** we have prepared an *Report.ipynb* notebook. The report should act as a summary of your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include all **parts** in the report!\n",
    "\n",
    "\n",
    "Good luck!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Hacky solution to acces the global utils package\n",
    "import sys,os\n",
    "sys.path.append(os.path.dirname(os.path.realpath('')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# local modules\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.progressbar import LitProgressBar\n",
    "from utils.dataset import ClassificationDataset\n",
    "from utils.model import Model\n",
    "from config import LabConfig\n",
    "from collections import OrderedDict\n",
    "from utils import plot\n",
    "import pprint\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchmetrics import Precision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CLASSES': ['square', 'triangle', 'circular', 'background'],\n",
       " 'fineGrained': False,\n",
       " 'NUM_CLASSES': 4,\n",
       " 'training_img_dir': '../data/FlyingObjectDataset_10K/training',\n",
       " 'validation_img_dir': '../data/FlyingObjectDataset_10K/validation',\n",
       " 'testing_img_dir': '../data/FlyingObjectDataset_10K/testing',\n",
       " 'SEED': 420,\n",
       " 'GPU': 0,\n",
       " 'IMAGE_WIDTH': 128,\n",
       " 'IMAGE_HEIGHT': 128,\n",
       " 'IMAGE_CHANNEL': 3,\n",
       " 'NUM_WORKERS': 4,\n",
       " 'BATCH_SIZE': 32,\n",
       " 'TENSORBORD_DIR': 'logs/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = LabConfig()\n",
    "cfg.todict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Task\n",
    "First we present an example task to get an idea of the implementation and how to structure the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "First load the dataloaders for three datasets; train, validation and test. Feel free to test different augmentations, more can be found at the [pytorch doc](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "Note that ToTensor and Rezise are required to reshape and transform the images correct. We do not want to apply augmentation to the test_transform that are applied on the validation and test dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), \n",
    "])\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), #, \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(ClassificationDataset(cfg.training_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=train_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "valid_dataloader = DataLoader(ClassificationDataset(cfg.validation_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(ClassificationDataset(cfg.testing_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise data\n",
    "To get an idea of the dataset we visualise the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (32, 3, 128, 128) torch.float32 0.0 1.0\n",
      "y (32,) torch.int64 0 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x201.6 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_x, t_y = next(iter(train_dataloader))\n",
    "print(f\"x {tuple(t_x.shape)} {t_x.dtype} {t_x.min()} {t_x.max()}\")\n",
    "print(f\"y {tuple(t_y.shape)} {t_y.dtype} {t_y.min()} {t_y.max()}\")\n",
    "plot.Classification.data(t_x, t_y, nimages=10,nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Set to true to visualise statistics of the data\n",
    "    plot.show_statistics(cfg.training_img_dir, fineGrained=cfg.fineGrained, title=\" Training Data Statistics \")\n",
    "    plot.show_statistics(cfg.validation_img_dir, fineGrained=cfg.fineGrained, title=\" Validation Data Statistics \")\n",
    "    plot.show_statistics(cfg.testing_img_dir, fineGrained=cfg.fineGrained, title=\" Testing Data Statistics \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "Here is an simple architecture to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 32)\n",
    "        self.conv_layer2 = self._conv_layer_set(32, 64)\n",
    "        self.fc1 = nn.Linear(64*input_shape[1]//4*input_shape[1]//4, 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "       \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "config = {\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(), # error function\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:730: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [13/339] {'loss': '7.92'}\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1805: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [339/339] {'loss': '0.0306'}}}\r"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=1,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[LitProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "# Train with the training and validation data- \n",
    "trainer.fit(\n",
    "    modelObj, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network on the test dataset\n",
    "To test the performance for a qualitative estimation we can plot the input, target and the models prediction. This is a good approach to see the performance and understand if the model is close to a correct decision. However, for big data, we probobly want to focus on a qualitative estimation. Therefore we can analyse **Tensorboard** logs to get a better understanding of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterable from the test dataset\n",
    "iter_dataloader = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch from the test dataset and predict!\n",
    "X, Y = next(iter_dataloader)\n",
    "preds = torch.argmax(modelObj.predict_step(X,0,0),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10\n",
    "df_result = pd.DataFrame({\n",
    "    'Ground Truth': Y[:n_test],\n",
    "    'Predicted label': preds[:n_test]})\n",
    "display(df_result.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.Classification.results(X, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "**TODO:** Does a high accuracy impy a good model, motivate your answer.\n",
    "\n",
    "**TODO:** Find an alternative metric which can show similar or better precision than accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "x_train, y_train = next(iter(train_dataloader))\n",
    "train_preds = torch.argmax(modelObj.predict_step(x_train, 0, 0), dim=1)\n",
    "\n",
    "x_test, y_test = next(iter(test_dataloader))\n",
    "preds = torch.argmax(modelObj.predict_step(x_test, 0, 0), dim=1)\n",
    "\n",
    "print('train acc: ' + str(np.mean(y_train.numpy() == train_preds.numpy())*100))\n",
    "print('test acc: ' + str(np.mean(y_test.numpy() == preds.numpy())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "precision = Precision(num_classes=cfg.todict()['NUM_CLASSES'])\n",
    "\n",
    "print(precision(train_preds, y_train))\n",
    "print(precision(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Modify the architecture of the SimpleModel to further increase the performance. Remember that very deep network allow the network to learn many features but if the dataset is to small the model will underfit. A simple dataset should not require a very deep network to learn good features.\n",
    "\n",
    "**TODO:** Modify the SimpleModel architecture. Force the network to overfit. How bad performance can you get from the network?\n",
    "\n",
    "**TODO:** Modify the SimpleModel and increase the complexity a little. Does the performance improve? If not, did you modify it to much or to little?\n",
    "\n",
    "**TODO:** Modify the SimpleModel architecture. Now combine the hyperparameter tuning and modification of the architecture to reach a performance that is close to the truth images. Explain in detail why the change was applied and if it improved the model a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "config = {\n",
    "    'drop':0.5,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        conv_layers = 3\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 24)\n",
    "        self.conv_layer2 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer3 = self._conv_layer_set(24, 40)\n",
    "        \n",
    "                \n",
    "        ## Look up how width and height should change between convolutional layers. \n",
    "        \n",
    "        self.fc1 = nn.Linear(40*input_shape[1]//np.power(2, conv_layers)*input_shape[1]//np.power(2, conv_layers), 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        \n",
    "        out = self.conv_layer1(x)      \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.conv_layer3(out) \n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "                               \n",
    "        \n",
    "        return out\n",
    "    \n",
    "def train():\n",
    "    # Load model\n",
    "    modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "     \n",
    "    # Setup trainer\n",
    "    trainer = pl.Trainer(\n",
    "                max_epochs=config['max_epochs'], \n",
    "                gpus=cfg.GPU,\n",
    "                logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "                callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "                progress_bar_refresh_rate=1,\n",
    "                weights_summary=None,\n",
    "                num_sanity_val_steps=10,   \n",
    "            )\n",
    "\n",
    "    trainer.fit(\n",
    "        modelObj, \n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    );\n",
    "    \n",
    "config['max_epochs'] = 10\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "config = {\n",
    "    'drop':0.5,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        conv_layers = 5\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 24)\n",
    "        self.conv_layer2 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer3 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer4 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer5 = self._conv_layer_set(24, 40)\n",
    "        \n",
    "                \n",
    "        ## Look up how width and height should change between convolutional layers. \n",
    "        \n",
    "        self.fc1 = nn.Linear(40*input_shape[1]//np.power(2, conv_layers)*input_shape[1]//np.power(2, conv_layers), 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=10, padding=5)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        \n",
    "        out = self.conv_layer1(x)      \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.conv_layer5(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "                               \n",
    "        \n",
    "        return out\n",
    "    \n",
    "def train():\n",
    "    # Load model\n",
    "    modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "     \n",
    "    # Setup trainer\n",
    "    trainer = pl.Trainer(\n",
    "                max_epochs=config['max_epochs'], \n",
    "                gpus=1,\n",
    "                logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "                callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "                progress_bar_refresh_rate=1,\n",
    "                weights_summary=None, # Can be None, top or full\n",
    "                num_sanity_val_steps=10,   \n",
    "            )\n",
    "\n",
    "    trainer.fit(\n",
    "        modelObj, \n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    );\n",
    "    \n",
    "config['max_epochs'] = 5\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "config = {\n",
    "    'drop':0.5,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        conv_layers = 3\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 24) # 2\n",
    "        self.conv_layer2 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer3 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer4 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer5 = self._conv_layer_set(24, 40)\n",
    "        \n",
    "                \n",
    "        ## Look up how width and height should change between convolutional layers. \n",
    "        \n",
    "        self.fc1 = nn.Linear(40*input_shape[1]//np.power(2, conv_layers)*input_shape[1]//np.power(2, conv_layers), 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=10, padding=5)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        \n",
    "        out = self.conv_layer1(x)      \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.conv_layer5(out) \n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "                               \n",
    "        \n",
    "        return out\n",
    "    \n",
    "def train():\n",
    "    # Load model\n",
    "    modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "     \n",
    "    # Setup trainer\n",
    "    trainer = pl.Trainer(\n",
    "                max_epochs=config['max_epochs'], \n",
    "                gpus=1,\n",
    "                logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "                callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "                progress_bar_refresh_rate=1,\n",
    "                weights_summary=None, # Can be None, top or full\n",
    "                num_sanity_val_steps=10,   \n",
    "            )\n",
    "\n",
    "    trainer.fit(\n",
    "        modelObj, \n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    );\n",
    "    \n",
    "config['max_epochs'] = 10\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "### Task 1\n",
    "From the example approach we can see that the network performed very poorly. For the network to be consider \"good\" the truth images should match the predicted images. If the architecture can learn but is unstable (check loss/epoch in tensorboard), it is possible to tune the parameters of the network. This mostly involves changing the learning rate, optimizers, loss function etc. to better learn features. A network that have a to high learning rate create a increase in variance of the network weights which can make the network unstable.\n",
    "\n",
    "\n",
    "**TODO:** Perform hyperparameter tuning. Explain in detail why the parameters was changed and why it is considered \"better\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        nr_conv_layers = 3\n",
    "        \n",
    "        conv_layers = []\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 32) # 2 \n",
    "        self.conv_layer2 = self._conv_layer_set(32, 128) \n",
    "        self.conv_layer3 = self._conv_layer_set(128, 150)\n",
    "        conv_layers.extend([self.conv_layer1, self.conv_layer2, self.conv_layer3]) #, self.conv_layer4])\n",
    "        \n",
    "        self.convolve = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        print(self.convolve)\n",
    "        \n",
    "        \n",
    "        feedforward = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(150*input_shape[1]//np.power(2, nr_conv_layers)*input_shape[1]//np.power(2, nr_conv_layers), 150) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(150, num_classes)\n",
    "        self.drop = nn.Dropout(0.75)\n",
    "        \n",
    "        \n",
    "        feedforward.append(self.fc1)\n",
    "        feedforward.append(self.drop)\n",
    "        feedforward.append(self.fc2)\n",
    "            \n",
    "        \n",
    "        self.ffd = nn.Sequential(*feedforward)\n",
    "        print(self.ffd)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        out = self.convolve(x)\n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.ffd(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "config = {\n",
    "    'drop':0.75,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.001,\n",
    "            'weight_decay': 0.0001\n",
    "\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config) \n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=1,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "\n",
    "trainer.fit(\n",
    "    modelObj, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(iter(test_dataloader))\n",
    "preds = torch.argmax(modelObj.predict_step(x_test, 0, 0), dim=1)\n",
    "print('test acc: ' + str(np.mean(y_test.numpy() == preds.numpy())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "**TODO:** Test if data augmentation help. Note that if we want to apply augmentation we need to make sure that the input and target perform the same augmentation. Otherwise, the data will not be correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Did data augmentation improve the model? \\\n",
    "**Question:** What do you think have the greatest impact on the performance, why? \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(0.3),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), \n",
    "])\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), #, \n",
    "])\n",
    "\n",
    "train_dataloader = DataLoader(ClassificationDataset(cfg.training_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=train_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "valid_dataloader = DataLoader(ClassificationDataset(cfg.validation_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(ClassificationDataset(cfg.testing_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config) \n",
    "\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=1,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None,\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "\n",
    "trainer.fit(\n",
    "    modelObj, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(iter(test_dataloader))\n",
    "preds = torch.argmax(modelObj.predict_step(x_test, 0, 0), dim=1)\n",
    "print('test acc: ' + str(np.mean(y_test.numpy() == preds.numpy())*100))\n",
    "\n",
    "confuTst = torchmetrics.functional.confusion_matrix(preds.detach().cpu(),Y.int().detach().cpu(), cfg.NUM_CLASSES)\n",
    "\n",
    "plot.confusion_matrix(cm = confuTst.numpy(), \n",
    "                      normalize = False,\n",
    "                      target_names = cfg.CLASSES,\n",
    "                      title = 'Test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ccf88e37874d44b4dfe33c31e1bb4a10ca4e414e0a68744582aebd290f71bcd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
