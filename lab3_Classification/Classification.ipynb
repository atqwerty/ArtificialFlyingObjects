{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Exercise III:<br> Image Classification using CNNs</h1></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the *fourth* lab for Deep Learning!\n",
    "\n",
    "In this lab an CNN network to classify RGB images. Image classification refers to classify classes from images. This labs the *dataset* consist of multiple images where each image have a target label for classification.\n",
    "\n",
    "All **tasks** include **TODO's** thare are expected to be done before the deadline. The highlighted **Question's** should be answered in the report. Keep the answers separated so it is easy to read for the grading. Some sections include asserts or an expected result to give a and expected results are given. Some sections does not contain any **TODO's** but is good to understand them. \n",
    "\n",
    "For the **report** we have prepared an *Report.ipynb* notebook. The report should act as a summary of your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include all **parts** in the report!\n",
    "\n",
    "\n",
    "Good luck!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Hacky solution to acces the global utils package\n",
    "import sys,os\n",
    "sys.path.append(os.path.dirname(os.path.realpath('')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# local modules\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.progressbar import LitProgressBar\n",
    "from utils.dataset import ClassificationDataset\n",
    "from utils.model import Model\n",
    "from config import LabConfig\n",
    "from collections import OrderedDict\n",
    "from utils import plot\n",
    "import pprint\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchmetrics import Precision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CLASSES': ['square', 'triangle', 'circular', 'background'],\n",
       " 'fineGrained': False,\n",
       " 'NUM_CLASSES': 4,\n",
       " 'training_img_dir': '../data/FlyingObjectDataset_10K/training',\n",
       " 'validation_img_dir': '../data/FlyingObjectDataset_10K/validation',\n",
       " 'testing_img_dir': '../data/FlyingObjectDataset_10K/testing',\n",
       " 'SEED': 420,\n",
       " 'GPU': 0,\n",
       " 'IMAGE_WIDTH': 128,\n",
       " 'IMAGE_HEIGHT': 128,\n",
       " 'IMAGE_CHANNEL': 3,\n",
       " 'NUM_WORKERS': 4,\n",
       " 'BATCH_SIZE': 32,\n",
       " 'TENSORBORD_DIR': 'logs/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = LabConfig()\n",
    "cfg.todict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Task\n",
    "First we present an example task to get an idea of the implementation and how to structure the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data\n",
    "First load the dataloaders for three datasets; train, validation and test. Feel free to test different augmentations, more can be found at the [pytorch doc](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "Note that ToTensor and Rezise are required to reshape and transform the images correct. We do not want to apply augmentation to the test_transform that are applied on the validation and test dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), \n",
    "])\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), #, \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(ClassificationDataset(cfg.training_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=train_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "valid_dataloader = DataLoader(ClassificationDataset(cfg.validation_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(ClassificationDataset(cfg.testing_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise data\n",
    "To get an idea of the dataset we visualise the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (32, 3, 128, 128) torch.float32 0.0 1.0\n",
      "y (32,) torch.int64 0 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x201.6 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_x, t_y = next(iter(train_dataloader))\n",
    "print(f\"x {tuple(t_x.shape)} {t_x.dtype} {t_x.min()} {t_x.max()}\")\n",
    "print(f\"y {tuple(t_y.shape)} {t_y.dtype} {t_y.min()} {t_y.max()}\")\n",
    "plot.Classification.data(t_x, t_y, nimages=10,nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Set to true to visualise statistics of the data\n",
    "    plot.show_statistics(cfg.training_img_dir, fineGrained=cfg.fineGrained, title=\" Training Data Statistics \")\n",
    "    plot.show_statistics(cfg.validation_img_dir, fineGrained=cfg.fineGrained, title=\" Validation Data Statistics \")\n",
    "    plot.show_statistics(cfg.testing_img_dir, fineGrained=cfg.fineGrained, title=\" Testing Data Statistics \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "Here is an simple architecture to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 32)\n",
    "        self.conv_layer2 = self._conv_layer_set(32, 64)\n",
    "        self.fc1 = nn.Linear(64*input_shape[1]//4*input_shape[1]//4, 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "       \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "config = {\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(), # error function\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/stud/d/denmar20/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:730: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n you tried to log -1 which is not currently supported. Try a dict or a scalar/tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mlog_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# todo: specify the possible exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/loggers/base.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDummyExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/loggers/base.py\u001b[0m in \u001b[0;36mget_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m_get_file_writer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             self.file_writer = FileWriter(self.log_dir, self.max_queue,\n\u001b[0m\u001b[1;32m    252\u001b[0m                                           self.flush_secs, self.filename_suffix)\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         self.event_writer = EventFileWriter(\n\u001b[0m\u001b[1;32m     62\u001b[0m             log_dir, max_queue, flush_secs, filename_suffix)\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/tensorboard/summary/writer/event_file_writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         self._file_name = (\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/tensorboard/lazy.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'io'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_300159/2142367294.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         )\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train with the training and validation data-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m trainer.fit(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodelObj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    733\u001b[0m             )\n\u001b[1;32m    734\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         )\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \"\"\"\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;31m# plugin will setup fitting (e.g. ddp will launch child processes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_checkpoint_after_pre_dispatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_pre_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_log_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_log_hyperparams\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhparams_initial\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mlog_hyperparams\u001b[0;34m(self, params, metrics)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deeplearn/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mlog_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n you tried to log {v} which is not currently supported. Try a dict or a scalar/tensor.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \n you tried to log -1 which is not currently supported. Try a dict or a scalar/tensor."
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=cfg.GPU,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[LitProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "# Train with the training and validation data- \n",
    "trainer.fit(\n",
    "    modelObj, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network on the test dataset\n",
    "To test the performance for a qualitative estimation we can plot the input, target and the models prediction. This is a good approach to see the performance and understand if the model is close to a correct decision. However, for big data, we probobly want to focus on a qualitative estimation. Therefore we can analyse **Tensorboard** logs to get a better understanding of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterable from the test dataset\n",
    "iter_dataloader = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch from the test dataset and predict!\n",
    "X, Y = next(iter_dataloader)\n",
    "preds = torch.argmax(modelObj.predict_step(X,0,0),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10\n",
    "df_result = pd.DataFrame({\n",
    "    'Ground Truth': Y[:n_test],\n",
    "    'Predicted label': preds[:n_test]})\n",
    "display(df_result.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.Classification.results(X, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "**TODO:** Does a high accuracy impy a good model, motivate your answer.\n",
    "\n",
    "**TODO:** Find an alternative metric which can show similar or better precision than accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "x_train, y_train = next(iter(train_dataloader))\n",
    "train_preds = torch.argmax(modelObj.predict_step(x_train, 0, 0), dim=1)\n",
    "\n",
    "x_test, y_test = next(iter(test_dataloader))\n",
    "preds = torch.argmax(modelObj.predict_step(x_test, 0, 0), dim=1)\n",
    "\n",
    "print('train acc: ' + str(np.mean(y_train.numpy() == train_preds.numpy())*100))\n",
    "print('test acc: ' + str(np.mean(y_test.numpy() == preds.numpy())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "precision = Precision(num_classes=cfg.todict()['NUM_CLASSES'])\n",
    "\n",
    "print(precision(train_preds, y_train))\n",
    "print(precision(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Modify the architecture of the SimpleModel to further increase the performance. Remember that very deep network allow the network to learn many features but if the dataset is to small the model will underfit. A simple dataset should not require a very deep network to learn good features.\n",
    "\n",
    "**TODO:** Modify the SimpleModel architecture. Force the network to overfit. How bad performance can you get from the network?\n",
    "\n",
    "**TODO:** Modify the SimpleModel and increase the complexity a little. Does the performance improve? If not, did you modify it to much or to little?\n",
    "\n",
    "**TODO:** Modify the SimpleModel architecture. Now combine the hyperparameter tuning and modification of the architecture to reach a performance that is close to the truth images. Explain in detail why the change was applied and if it improved the model a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "config = {\n",
    "    'drop':0.5,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        conv_layers = 3\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 24)\n",
    "        self.conv_layer2 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer3 = self._conv_layer_set(24, 40)\n",
    "        \n",
    "                \n",
    "        ## Look up how width and height should change between convolutional layers. \n",
    "        \n",
    "        self.fc1 = nn.Linear(40*input_shape[1]//np.power(2, conv_layers)*input_shape[1]//np.power(2, conv_layers), 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        \n",
    "        out = self.conv_layer1(x)      \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.conv_layer3(out) \n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "                               \n",
    "        \n",
    "        return out\n",
    "    \n",
    "def train():\n",
    "    # Load model\n",
    "    modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "     \n",
    "    # Setup trainer\n",
    "    trainer = pl.Trainer(\n",
    "                max_epochs=config['max_epochs'], \n",
    "                gpus=cfg.GPU,\n",
    "                logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "                callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "                progress_bar_refresh_rate=1,\n",
    "                weights_summary=None,\n",
    "                num_sanity_val_steps=10,   \n",
    "            )\n",
    "\n",
    "    trainer.fit(\n",
    "        modelObj, \n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    );\n",
    "    \n",
    "config['max_epochs'] = 10\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "config = {\n",
    "    'drop':0.5,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        conv_layers = 5\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 24)\n",
    "        self.conv_layer2 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer3 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer4 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer5 = self._conv_layer_set(24, 40)\n",
    "        \n",
    "                \n",
    "        ## Look up how width and height should change between convolutional layers. \n",
    "        \n",
    "        self.fc1 = nn.Linear(40*input_shape[1]//np.power(2, conv_layers)*input_shape[1]//np.power(2, conv_layers), 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=10, padding=5)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        \n",
    "        out = self.conv_layer1(x)      \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.conv_layer5(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "                               \n",
    "        \n",
    "        return out\n",
    "    \n",
    "def train():\n",
    "    # Load model\n",
    "    modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "     \n",
    "    # Setup trainer\n",
    "    trainer = pl.Trainer(\n",
    "                max_epochs=config['max_epochs'], \n",
    "                gpus=1,\n",
    "                logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "                callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "                progress_bar_refresh_rate=1,\n",
    "                weights_summary=None, # Can be None, top or full\n",
    "                num_sanity_val_steps=10,   \n",
    "            )\n",
    "\n",
    "    trainer.fit(\n",
    "        modelObj, \n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    );\n",
    "    \n",
    "config['max_epochs'] = 5\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "\n",
    "config = {\n",
    "    'drop':0.5,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.005,\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        conv_layers = 3\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 24) # 2\n",
    "        self.conv_layer2 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer3 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer4 = self._conv_layer_set(24, 24)\n",
    "        self.conv_layer5 = self._conv_layer_set(24, 40)\n",
    "        \n",
    "                \n",
    "        ## Look up how width and height should change between convolutional layers. \n",
    "        \n",
    "        self.fc1 = nn.Linear(40*input_shape[1]//np.power(2, conv_layers)*input_shape[1]//np.power(2, conv_layers), 64) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=10, padding=5)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        \n",
    "        out = self.conv_layer1(x)      \n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.conv_layer5(out) \n",
    "        \n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "                               \n",
    "        \n",
    "        return out\n",
    "    \n",
    "def train():\n",
    "    # Load model\n",
    "    modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config)\n",
    "     \n",
    "    # Setup trainer\n",
    "    trainer = pl.Trainer(\n",
    "                max_epochs=config['max_epochs'], \n",
    "                gpus=1,\n",
    "                logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "                callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "                progress_bar_refresh_rate=1,\n",
    "                weights_summary=None, # Can be None, top or full\n",
    "                num_sanity_val_steps=10,   \n",
    "            )\n",
    "\n",
    "    trainer.fit(\n",
    "        modelObj, \n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader\n",
    "    );\n",
    "    \n",
    "config['max_epochs'] = 10\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "### Task 1\n",
    "From the example approach we can see that the network performed very poorly. For the network to be consider \"good\" the truth images should match the predicted images. If the architecture can learn but is unstable (check loss/epoch in tensorboard), it is possible to tune the parameters of the network. This mostly involves changing the learning rate, optimizers, loss function etc. to better learn features. A network that have a to high learning rate create a increase in variance of the network weights which can make the network unstable.\n",
    "\n",
    "\n",
    "**TODO:** Perform hyperparameter tuning. Explain in detail why the parameters was changed and why it is considered \"better\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,num_channels:int=4, num_classes:int=3, input_shape=(10,10),**kwargs):\n",
    "        super().__init__()\n",
    "        nr_conv_layers = 3\n",
    "        \n",
    "        conv_layers = []\n",
    "        self.conv_layer1 = self._conv_layer_set(num_channels, 32) # 2 \n",
    "        self.conv_layer2 = self._conv_layer_set(32, 128) \n",
    "        self.conv_layer3 = self._conv_layer_set(128, 150)\n",
    "        conv_layers.extend([self.conv_layer1, self.conv_layer2, self.conv_layer3]) #, self.conv_layer4])\n",
    "        \n",
    "        self.convolve = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        print(self.convolve)\n",
    "        \n",
    "        \n",
    "        feedforward = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(150*input_shape[1]//np.power(2, nr_conv_layers)*input_shape[1]//np.power(2, nr_conv_layers), 150) # Calculated with the size. why //4\n",
    "        self.fc2 = nn.Linear(150, num_classes)\n",
    "        self.drop = nn.Dropout(0.75)\n",
    "        \n",
    "        \n",
    "        feedforward.append(self.fc1)\n",
    "        feedforward.append(self.drop)\n",
    "        feedforward.append(self.fc2)\n",
    "            \n",
    "        \n",
    "        self.ffd = nn.Sequential(*feedforward)\n",
    "        print(self.ffd)\n",
    "        \n",
    "    def _conv_layer_set(self, in_c, out_c):\n",
    "        conv_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv',nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)),\n",
    "            ('leakyrelu',nn.LeakyReLU()),\n",
    "            ('maxpool',nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        return conv_layer\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set 1\n",
    "        \n",
    "        out = self.convolve(x)\n",
    "        out = out.view(out.size(0), -1) # Flatten (batchsize, image size)\n",
    "        \n",
    "        out = self.ffd(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "config = {\n",
    "    'drop':0.75,\n",
    "    'optimizer':{\n",
    "        \"type\":torch.optim.Adam,\n",
    "        \"args\":{\n",
    "            \"lr\":0.001,\n",
    "            'weight_decay': 0.0001\n",
    "\n",
    "        }\n",
    "    },\n",
    "    'criterion':torch.nn.CrossEntropyLoss(),\n",
    "    'max_epochs':5,\n",
    "    \"train_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Train\"),\n",
    "    \"validation_metrics\":torchmetrics.MetricCollection([\n",
    "        torchmetrics.Accuracy(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "        torchmetrics.F1(num_classes=cfg.NUM_CLASSES,compute_on_step=False),\n",
    "    ],postfix=\"_Validation\")\n",
    "}\n",
    "\n",
    "modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config) \n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=1,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None, # Can be None, top or full\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "\n",
    "trainer.fit(\n",
    "    modelObj, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(iter(test_dataloader))\n",
    "preds = torch.argmax(modelObj.predict_step(x_test, 0, 0), dim=1)\n",
    "print('test acc: ' + str(np.mean(y_test.numpy() == preds.numpy())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "**TODO:** Test if data augmentation help. Note that if we want to apply augmentation we need to make sure that the input and target perform the same augmentation. Otherwise, the data will not be correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Did data augmentation improve the model? \\\n",
    "**Question:** What do you think have the greatest impact on the performance, why? \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.RandomHorizontalFlip(0.3),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), \n",
    "])\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)), #, \n",
    "])\n",
    "\n",
    "train_dataloader = DataLoader(ClassificationDataset(cfg.training_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=train_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "valid_dataloader = DataLoader(ClassificationDataset(cfg.validation_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(ClassificationDataset(cfg.testing_img_dir, cfg.CLASSES, img_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH),transform=test_transform),\n",
    "                        batch_size=cfg.BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=cfg.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelObj = Model(SimpleModel(num_classes=cfg.NUM_CLASSES, num_channels=cfg.IMAGE_CHANNEL, input_shape=(cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH)),**config) \n",
    "\n",
    "trainer = pl.Trainer(\n",
    "            max_epochs=config['max_epochs'], \n",
    "            gpus=1,\n",
    "            logger=pl.loggers.TensorBoardLogger(save_dir=cfg.TENSORBORD_DIR),\n",
    "            callbacks=[pl.callbacks.progress.TQDMProgressBar()],\n",
    "            progress_bar_refresh_rate=1,\n",
    "            weights_summary=None,\n",
    "            num_sanity_val_steps=10,   \n",
    "        )\n",
    "\n",
    "trainer.fit(\n",
    "    modelObj, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(iter(test_dataloader))\n",
    "preds = torch.argmax(modelObj.predict_step(x_test, 0, 0), dim=1)\n",
    "print('test acc: ' + str(np.mean(y_test.numpy() == preds.numpy())*100))\n",
    "\n",
    "confuTst = torchmetrics.functional.confusion_matrix(preds.detach().cpu(),Y.int().detach().cpu(), cfg.NUM_CLASSES)\n",
    "\n",
    "plot.confusion_matrix(cm = confuTst.numpy(), \n",
    "                      normalize = False,\n",
    "                      target_names = cfg.CLASSES,\n",
    "                      title = 'Test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ccf88e37874d44b4dfe33c31e1bb4a10ca4e414e0a68744582aebd290f71bcd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
