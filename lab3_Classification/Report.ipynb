{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0081d975-63e0-49e6-b0d7-a597288d6423",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7c81-20d8-48f8-85ce-e25d78cf81f1",
   "metadata": {},
   "source": [
    "**Name:** Markitanov Denis\\\n",
    "**Date:** 12.12.21\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is the 3dr lab of the Deep Learning course. This lab was focused on experiments and observations of Concolutiona Neural Network performance and behaviour in the context of image classification. Arcitecture, tuning and accuracy evaluation were studied in this lab\n",
    "\n",
    "## Result\n",
    "\n",
    "### Metrics:\n",
    "\n",
    "##### Task 1\n",
    "\n",
    "High accuracy does not imply the high quality of a model. In some cases it can be complete opposite. Such case is overfitting that usually shows excelent performance on the training data, but will drop with unseen data. However, in general, having 100% accurate model might be useless. In my personal experience, we had a similar course in bachelors. There was a simple example where its faster to write a robost if/else program to classify rather than spending time on the 100% accurate model. Professor motivated it with the human nature to make mistakes. Thus, if AIs purpose to imitate human activity, for example, then it must make mistakes as humans do. I've decided to add this example because it might be an interesting thought to think about.\n",
    "\n",
    "##### Task 2\n",
    "\n",
    "I have decided to go with torchmetrics Precision metrics as it is simple and works with true and false positives. In this excercise we were supposed to look for alternative metrics regardless of the result. And as seen in the result, train and test accuracy is the same as mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad4f5d",
   "metadata": {},
   "source": [
    "### Architecture changes\n",
    "\n",
    "##### Task 1\n",
    "\n",
    "I have decided to force overfitting with number of epochs. It is a slow process, considering that model converges with smaller difference with each epoch. I decided to go with only 10 epochs. The loss was slowly falling with each epoch and reached 0.00215 which is a \n",
    "\n",
    "##### Task 2\n",
    "\n",
    "\n",
    "##### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8572e",
   "metadata": {},
   "source": [
    "### Hypertuning\n",
    "\n",
    "I've been playing with different parameters that include convolutional layers, learning rate, l2 regularization, kernel size and padding, output channels and so on. However, during my test the accuracy dropped to 65% accuracy. Thus, I was trying more different values for parameters. But even after that, the change in accuracy was different only by half of percent. The only parameter that I decided to not change was the number of epochs. For a week now I can't install gpu drivers and I was doing all my work on CPU, while trying to find a solution (what guys suggested on slack doesnt work as the package downloading is bugging out almost every time). I will continue on trying to install required dependencies to run my implementations on GPUs in following labs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e56548",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "##### Task 1\n",
    "\n",
    "For me it didnt improve the model. The accuracy dropped. I was also suggested by a friend to use the confusion matrix, but results were still worse than without augmentation. I retuned parameters of the model and ran it again. The accuracy increased to 53%, which is not a good result, but it is a good progress. Previous result was 33% and it changed to over 20% increase with augmentation. The result of tuned model is not the best. However, the point of this excercise was to see how augmentation affects the result. And while result is still lower than tuned model, the increase in 20% between 2 tuned models with same accuracy is a valid observation for this exercise.\n",
    "\n",
    "##### Task 2\n",
    "\n",
    "From what I can tell, the main impact on performance comes from the architecture of the model. CNN in our example works with images, using pixels (or sometimes regions) as inputs. Thus, shrinking or extending nodes might greatly affect the result. The next are parameter, CNN like NNs in previous lab comes with set of parameter, by which we can increase accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e0913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
