{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0081d975-63e0-49e6-b0d7-a597288d6423",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Summary your findings and motivate your choice of approach. A better motivation show your understanding of the lab. Dont forget to include the result from part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7c81-20d8-48f8-85ce-e25d78cf81f1",
   "metadata": {},
   "source": [
    "**Name:** Markitanov Denis\\\n",
    "**Date:** 12.12.21\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is the 3dr lab of the Deep Learning course. This lab was focused on experiments and observations of Concolutiona Neural Network performance and behaviour in the context of image classification. Arcitecture, tuning and accuracy evaluation were studied in this lab\n",
    "\n",
    "## Result\n",
    "\n",
    "### Metrics:\n",
    "\n",
    "##### Task 1\n",
    "\n",
    "High accuracy does not imply the high quality of a model. In some cases it can be complete opposite. Such case is overfitting that usually shows excelent performance on the training data, but will drop with unseen data. However, in general, having 100% accurate model might be useless. In my personal experience, we had a similar course in bachelors. There was a simple example where its faster to write a robost if/else program to classify rather than spending time on the 100% accurate model. Professor motivated it with the human nature to make mistakes. Thus, if AIs purpose to imitate human activity, for example, then it must make mistakes as humans do. I've decided to add this example because it might be an interesting thought to think about.\n",
    "\n",
    "##### Task 2\n",
    "\n",
    "I have decided to go with torchmetrics Precision metrics as it is simple and works with true and false positives. In this excercise we were supposed to look for alternative metrics regardless of the result. And as seen in the result, train and test accuracy is the same as mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad4f5d",
   "metadata": {},
   "source": [
    "### Architecture changes\n",
    "\n",
    "##### Task 1\n",
    "\n",
    "I have decided to force overfitting with kernel size and padding. I had to use trial and error as different parameters did not fit. Original 3x3 kernel size and 1 padding are widely used (according to several articles from medium). However, anything after 5 is considered big kernel. Thus I was tuning kernel size beyond 5 and different paddings. My final sizes are 11x11 kernel size and 5 padding.\n",
    "\n",
    "The initial test accuracy showed 34%, while being overfit with 100% accuracy for the train data. Results with my architecture showed the increase in test accuracy by 3.5% (37.5%) and decrease of train data accuracy.\n",
    "\n",
    "##### Task 2\n",
    "\n",
    "For this task, I used initial kernel size and padding (3 and 1 accordingly) and increased numbers of convolutional layers to 4 with different numbers of channels. The results are very similar to the original with 34% accuracy for the test data and 100% accuracy for the train. However, I've tried with different parameters before my GPU problem (which I explained in Slack) and numbers were different, going up to 60% accuracy for the test.\n",
    "\n",
    "\n",
    "##### Task 3\n",
    "\n",
    "I've combined kernel size and padding from 1st architecture with more layers from the second one. I've also increased number of convolutional layers to 6. As a result I was able to obtain 87.5% accuracy for the test samples. Which is not close to the original images, but more than twice as big as previous results.\n",
    "\n",
    "Increasing the number of layers, slowly but surely, will cause the model to overfit. According to the answer here (https://datascience.stackexchange.com/questions/52889/does-increasing-kernel-size-in-a-cnn-result-in-higher-accuracy-on-the-training-s) there is no direct relation between kernel size and overfitting. However, as explained in the answer, bigger kernels might miss features that 3x3 can detect and vice versa. It highly depends on the data. And in our case, it might not be the issue that increases accuracy, let alone overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8572e",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "For hyperparamenter tuning task I've used the same model with different drop (0.75), learning rate (0.0001), l2 (0.0001), and increased number of epochs to 50. The test accuracy achieved 87.5% as the original model. I believe that changing output channels and convolutional layers might increase the accuracy. However, I don't know if it counts as hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e56548",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "##### Task 1\n",
    "\n",
    "For me it didnt improve the model. 87.5% as before. I've used the transforms with random horizontal flip and resizing. But it didn't help much\n",
    "\n",
    "##### Task 2\n",
    "\n",
    "From what I can tell, the main impact on performance comes from the architecture of the model. CNN in our example works with images, using pixels (or sometimes regions) as inputs. Thus, the amount of convolutional layers showed a greater impact on the accuracy. The next are parameters, CNN like NNs in previous lab comes with set of parameter, by which we can increase accuracy. However, as I've said before, in our example there was no difference between tuned and not tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3927724",
   "metadata": {},
   "source": [
    "### The tensorboard logs for each experiment are sorted in directories accordingly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
